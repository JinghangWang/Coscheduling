From e1146906a40373e3f9883f62cc68bcfb09e5db0e Mon Sep 17 00:00:00 2001
From: Peter Dinda <pdinda@northwestern.edu>
Date: Fri, 23 Jun 2017 10:33:35 -0500
Subject: [PATCH 15/20] Thread, wait queue, and scheduler-interaction
 rewrites/bug fixes

- Uses preemption facility to avoid potential race conditions in
  wait-queue-related operations, including fork/join
- Updates special scheduler invocations (handle_special_switch)
  to manage preemption
- Special scheduler invocations now allow passing a lock
  to be released
- Rewrites of sleep/wake, exit/join paths to take advantage of
  the above
- Fixes to thread fork, including cloning of FP state
- Some generalization of FP state to allow future use of xsave
  and exposure of FP state save/load outside of low-level code
- Fix to bug in init/smp_ap_entry code paths that would have
  allow their thread stacks to get underrun, corrupting other
  heap memory.
- Updates to code dependeng on wait queues (except for vc
  which is in a separate commit)
---
 include/nautilus/cpu_state.h |  14 ++
 include/nautilus/scheduler.h |  19 +-
 include/nautilus/thread.h    |  79 +++++---
 src/asm/thread_lowlevel.S    |  18 +-
 src/nautilus/condvar.c       |  10 +-
 src/nautilus/dev.c           |   2 +-
 src/nautilus/scheduler.c     | 161 ++++++++++++++---
 src/nautilus/thread.c        | 421 +++++++++++++++++++++++++++----------------
 src/nautilus/timer.c         |   8 +-
 9 files changed, 507 insertions(+), 225 deletions(-)

diff --git a/include/nautilus/cpu_state.h b/include/nautilus/cpu_state.h
index a034cc0..f8ec31e 100644
--- a/include/nautilus/cpu_state.h
+++ b/include/nautilus/cpu_state.h
@@ -65,6 +65,20 @@ static inline void preempt_enable()
     }
 }
 
+// make cpu preeemptible once again
+// this should only be called by scheduler in handling
+// special context switch requests
+static inline void preempt_reset() 
+{
+    void *base = __cpu_state_get_cpu();
+    if (base) {
+	// per-cpu functional
+	__sync_fetch_and_and((uint16_t *)(base+10),0);
+    } else {
+	// per-cpu is not running, so we are not going to get preempted anyway
+    }
+}
+
 static inline int preempt_is_disabled()
 {
     void *base = __cpu_state_get_cpu();
diff --git a/include/nautilus/scheduler.h b/include/nautilus/scheduler.h
index a8e7ce4..db539ca 100644
--- a/include/nautilus/scheduler.h
+++ b/include/nautilus/scheduler.h
@@ -132,19 +132,26 @@ void   nk_sched_kick_cpu(int cpu);
 // Put the thread to sleep / awaken it
 // these signal the scheduler that the thread is now on a 
 // non-scheduler queue (sleep) or is to be returned to a scheduler 
-// queue (awaken)
-void    nk_sched_sleep();
+// queue (awaken)   
+// the lock_to_release will be released once the scheduling pass is complete
+// this allows the user code to manage races between its own abstractions
+// (e.g., wait queues) and the scheduling process
+// nk_sched_sleep will also renable preemption on the core before switching to
+// the new thread
+void    nk_sched_sleep(spinlock_t *lock_to_release);
 #define nk_sched_awaken(thread,cpu) nk_sched_make_runnable(thread,cpu,0)
 
 // Have the thread yield to another, if appropriate
-void              nk_sched_yield(void);
-#define           nk_sched_schedule() nk_sched_yield()
+void              nk_sched_yield(spinlock_t *lock_to_release);
+#define           nk_sched_schedule(lock_to_release) nk_sched_yield(lock_to_release)
 
 // Thread exit - will not return!
-void    nk_sched_exit();
+// does this have the same issue as sleep?
+void    nk_sched_exit(spinlock_t *lock_to_release);
 
 // clean up after detached threads
-void    nk_sched_reap();
+// normally will only execute if we have too many threads active
+void    nk_sched_reap(int unconditional);
 
 // return ns
 uint64_t nk_sched_get_realtime();
diff --git a/include/nautilus/thread.h b/include/nautilus/thread.h
index 16f7d77..81fc078 100644
--- a/include/nautilus/thread.h
+++ b/include/nautilus/thread.h
@@ -50,6 +50,7 @@ typedef void (*nk_thread_fun_t)(void * input, void ** output);
 typedef uint64_t nk_stack_size_t;
 
 
+// Create thread but do not launch it 
 int
 nk_thread_create (nk_thread_fun_t fun, 
 		  void * input,
@@ -59,9 +60,11 @@ nk_thread_create (nk_thread_fun_t fun,
 		  nk_thread_id_t * tid,
 		  int bound_cpu);  // -1 => not bound
 
+// Launch a previously created thread
 int
 nk_thread_run(nk_thread_id_t tid);
 
+// Create and launch a thread
 int
 nk_thread_start (nk_thread_fun_t fun, 
                  void * input,
@@ -71,19 +74,40 @@ nk_thread_start (nk_thread_fun_t fun,
                  nk_thread_id_t * tid,
                  int bound_cpu); // -1 => not bound
 
-int nk_thread_name(nk_thread_id_t tid, char *name);
-
+// fork the current thread 
+//   - parent is returned the tid of child
+//   - child is returned zero
+//   - child runs until it returns from the 
+//     current function, which returns into
+//     the thread cleanup logic instead of to
+//     the caller
 extern nk_thread_id_t nk_thread_fork(void);
 
+// Allow a child thread to set output explicitly
+// since it has no start function to which we can
+// pass an output pointer
 void nk_set_thread_fork_output(void * result);
-void nk_thread_exit(void * retval);
-void nk_thread_destroy(nk_thread_id_t t); /* like thread_kill */
-void nk_wait(nk_thread_id_t t);
+
+// Give a thread a name
+int nk_thread_name(nk_thread_id_t tid, char *name);
+
+// explicit yield (hides details of the scheduler)
 void nk_yield();
 
-void nk_wake_waiters(void);
+// Wait for a child or all children to exit
+// This includes forked threads
+// When joining all children, the optional
+// function consumes their output values
 int nk_join(nk_thread_id_t t, void ** retval);
-int nk_join_all_children(int (*)(void*));
+int nk_join_all_children(int (*output_consumer)(void *output));
+
+// called explictly or implicitly when a thread exits
+void nk_thread_exit(void * retval);
+
+// garbage collect an exited thread - typically called
+// by the reaper logic in the scheduler
+void nk_thread_destroy(nk_thread_id_t t);
+
 
 #ifndef __LEGION__
 nk_thread_id_t nk_get_tid(void);
@@ -102,7 +126,18 @@ int nk_tls_set(nk_tls_key_t key, const void * val);
 
 /********* INTERNALS ***********/
 
-#define FXSAVE_SIZE 512
+// Support both fxsave and xsave for FP state
+// The specific instruction used is determined in thread_lowlevel.S
+#define XSAVE_SIZE 4096           // guess - actually is extensible format
+                                  // current reasoning: 512 bytes for legacy
+                                  // 1KB + for AVX2 (32*512 bit)
+                                  // extra space
+#define XSAVE_ALIGN 64            // per Intel docs
+#define FXSAVE_SIZE 512           // per Intel docs
+#define FXSAVE_ALIGN 16           // per Intel docs
+
+#define FPSTATE_SIZE (XSAVE_SIZE>FXSAVE_SIZE ? XSAVE_SIZE : FXSAVE_SIZE)
+#define FPSTATE_ALIGN (XSAVE_ALIGN>FXSAVE_ALIGN ? XSAVE_ALIGN : FXSAVE_ALIGN)
 
 #define MAX_THREAD_NAME 32
 
@@ -115,19 +150,20 @@ int nk_tls_set(nk_tls_key_t key, const void * val);
 
 /* thread status */
 typedef enum {
-    NK_THR_INIT,
+    NK_THR_INIT=0,
     NK_THR_RUNNING, 
     NK_THR_WAITING,
     NK_THR_SUSPENDED, 
-    NK_THR_EXITED
+    NK_THR_EXITED,
 } nk_thread_status_t;
 
+
 typedef struct nk_queue nk_thread_queue_t;
 
 struct nk_thread {
-    uint64_t rsp; /* SHOULD NOT CHANGE POSITION */
-    void * stack; /* SHOULD NOT CHANGE POSITION */
-    uint16_t fpu_state_offset; /* SHOULD NOT CHANGE POSITION */
+    uint64_t rsp;              /* +0  SHOULD NOT CHANGE POSITION */
+    void * stack;              /* +8  SHOULD NOT CHANGE POSITION */
+    uint16_t fpu_state_offset; /* +16 SHOULD NOT CHANGE POSITION */
 
     nk_stack_size_t stack_size;
     unsigned long tid;
@@ -146,8 +182,6 @@ struct nk_thread {
     nk_thread_queue_t * waitq;
     nk_queue_entry_t wait_node;
 
-    nk_thread_queue_t * cur_run_q;
-    
     /* thread state */
     nk_thread_status_t status;
 
@@ -169,8 +203,8 @@ struct nk_thread {
 
     const void * tls[TLS_MAX_KEYS];
 
-    uint8_t fpu_state[FXSAVE_SIZE] __align(16);
-} __packed;
+    uint8_t fpu_state[FPSTATE_SIZE] __align(FPSTATE_ALIGN);
+} ;
 
 // internal thread representations
 typedef struct nk_thread nk_thread_t;
@@ -189,11 +223,12 @@ _nk_thread_init (nk_thread_t * t,
 
 nk_thread_queue_t * nk_thread_queue_create (void);
 void nk_thread_queue_destroy(nk_thread_queue_t * q);
-inline void nk_enqueue_thread_on_runq(nk_thread_t * t, int cpu);
-inline nk_thread_t* nk_dequeue_thread_from_runq(nk_thread_t * t);
-int nk_thread_queue_sleep(nk_thread_queue_t * q);
-int nk_thread_queue_wake_one(nk_thread_queue_t * q);
-int nk_thread_queue_wake_all(nk_thread_queue_t * q);
+
+void nk_thread_queue_sleep(nk_thread_queue_t * q);
+//  cond_check is condition (if any) to check atomically with enqueueing
+void nk_thread_queue_sleep_extended(nk_thread_queue_t * q, int (*cond_check)(void *state), void *state);
+void nk_thread_queue_wake_one(nk_thread_queue_t * q);
+void nk_thread_queue_wake_all(nk_thread_queue_t * q);
 
 struct nk_tls {
     unsigned seq_num;
diff --git a/src/asm/thread_lowlevel.S b/src/asm/thread_lowlevel.S
index 437f6a8..bf665cc 100644
--- a/src/asm/thread_lowlevel.S
+++ b/src/asm/thread_lowlevel.S
@@ -216,7 +216,6 @@ leaq 8(%rsp), %rsp ;                            // This instruction must not aff
 */	
 
 ENTRY(nk_thread_fork)
-    cli                         // turn interrupts off
 	REG_SAVE()                  // RBP and RSP will be handled in _thread_fork
 
 	// Note that callee will see us as part of the stack frame
@@ -228,7 +227,6 @@ ENTRY(nk_thread_fork)
 // different stacks and different rax return values
 ENTRY(_fork_return)
 	REG_LOAD_SKIP_RAX()         // we want the return produced by _thread_fork
-    sti                         // turn interrupts back on (this is actually unnecessary in the child but oh well)
 	retq                        // and back we go
 
 /*
@@ -261,6 +259,22 @@ ENTRY(nk_interrupt_like_trampoline)
 	// now we have 7 more words of crud to toss
 	leaq 56(%rsp), %rsp
 	retq			
+
+
+/*
+	Allow C-code to do FP saves/restores
+
+	nk_fp_save(destptr)
+	mk_fp_restore(srcptr)
+	
+*/
+ENTRY(nk_fp_save)
+	fxsave (%rdi)
+	ret
+
+ENTRY(nk_fp_restore)
+	fxrstor (%rdi)
+	ret
 	
 panic_str:
 .ascii "Stack corruption detected\12\0"
diff --git a/src/nautilus/condvar.c b/src/nautilus/condvar.c
index 00a96c4..fe82bf1 100644
--- a/src/nautilus/condvar.c
+++ b/src/nautilus/condvar.c
@@ -139,10 +139,7 @@ nk_condvar_signal (nk_condvar_t * c)
 
         DEBUG_PRINT("Condvar signaling on (%p)\n", (void*)c);
 
-        if (unlikely(nk_thread_queue_wake_one(c->wait_queue) != 0)) {
-            ERROR_PRINT("Could not signal on condvar\n");
-            return -1;
-        }
+        nk_thread_queue_wake_one(c->wait_queue);
 
     }
 
@@ -169,10 +166,7 @@ nk_condvar_bcast (nk_condvar_t * c)
         NK_UNLOCK(&c->lock);
 
         DEBUG_PRINT("Condvar broadcasting on (%p) (core=%u)\n", (void*)c, my_cpu_id());
-        if (unlikely(nk_thread_queue_wake_all(c->wait_queue) != 0)) {
-            ERROR_PRINT("Could not broadcast on condvar\n");
-            return -1;
-        }
+        nk_thread_queue_wake_all(c->wait_queue);
         return 0;
 
     }
diff --git a/src/nautilus/dev.c b/src/nautilus/dev.c
index 9b9ed24..036b0a9 100644
--- a/src/nautilus/dev.c
+++ b/src/nautilus/dev.c
@@ -140,7 +140,7 @@ void nk_dev_wait(struct nk_dev *d)
     } else {
 	// We are in a thread context and we will
 	// put ourselves to sleep
-	nk_thread_queue_sleep(d->waiting_threads);
+	nk_thread_queue_sleep_extended(d->waiting_threads, 0, 0);
     }
 }
 
diff --git a/src/nautilus/scheduler.c b/src/nautilus/scheduler.c
index a7561e9..7375ad8 100644
--- a/src/nautilus/scheduler.c
+++ b/src/nautilus/scheduler.c
@@ -54,16 +54,24 @@
 
 #define SANITY_CHECKS 0
 
+#define DUMP_THREAD_RT_STATE 0
+#define DUMP_SCHED_STATE     0
+
 #define INFO(fmt, args...) INFO_PRINT("Scheduler: " fmt, ##args)
 #define ERROR(fmt, args...) ERROR_PRINT("Scheduler: " fmt, ##args)
 
+
 #define DEBUG(fmt, args...)
 #define DEBUG_DUMP(rt,pre)
 #ifdef NAUT_CONFIG_DEBUG_SCHED
 #undef DEBUG
 #undef DEBUG_DUMP
 #define DEBUG(fmt, args...) DEBUG_PRINT("Scheduler: " fmt, ##args)
+#if DUMP_THREAD_RT_STATE
 #define DEBUG_DUMP(rt,pre) rt_thread_dump(rt,pre)
+#else
+#define DEBUG_DUMP(rt,pre) 
+#endif
 #endif
 
 
@@ -79,7 +87,7 @@
 #define UTIL_ONE 1000000ULL
 
 // Maximum number of threads within a priority queue or queue
-#define MAX_QUEUE NAUT_CONFIG_MAX_THREADS
+#define MAX_QUEUE (NAUT_CONFIG_MAX_THREADS)
 
 
 #define GLOBAL_LOCK_CONF uint8_t _global_flags=0
@@ -300,7 +308,15 @@ typedef struct nk_sched_percpu_state {
 #define REMOVE_APERIODIC(s,t) lottery_remove_aperiodic(s,t)
 #endif
 #define HAVE_APERIODIC(s) (!rt_queue_empty(&(s)->aperiodic))
+#ifdef NAUT_CONFIG_DEBUG_SCHED
+#if DUMP_SCHED_STATE
 #define DUMP_APERIODIC(s,p) rt_queue_dump(&(s)->aperiodic,p)
+#else
+#define DUMP_APERIODIC(s,p) 
+#endif
+#else
+#define DUMP_APERIODIC(s,p) 
+#endif
 #define PEEK_APERIODIC(s,k) rt_queue_peek(&(s)->aperiodic,k)
 #define SIZE_APERIODIC(s) ((s)->aperiodic.size)
 #else
@@ -310,7 +326,15 @@ typedef struct nk_sched_percpu_state {
 #define PEEK_APERIODIC(s,k) rt_priority_queue_peek(&(s)->aperiodic,k)
 #define SIZE_APERIODIC(s) ((s)->aperiodic.size)
 #define HAVE_APERIODIC(s) (!rt_priority_queue_empty(&(s)->aperiodic))
+#ifdef NAUT_CONFIG_DEBUG_SCHED
+#if DUMP_SCHED_STATE
 #define DUMP_APERIODIC(s,p) rt_priority_queue_dump(&(s)->aperiodic,p)
+#else
+#define DUMP_APERIODIC(s,p) 
+#endif
+#else
+#define DUMP_APERIODIC(s,p) 
+#endif
 #endif
 
 #define GET_NEXT_RT_PENDING(s)   rt_priority_queue_dequeue(&(s)->pending)
@@ -318,14 +342,30 @@ typedef struct nk_sched_percpu_state {
 #define REMOVE_RT_PENDING(s,t) rt_priority_queue_remove(&(s)->pending,t)
 #define HAVE_RT_PENDING(s) (!rt_priority_queue_empty(&(s)->pending))
 #define PEEK_RT_PENDING(s) (s->pending.threads[0])
+#ifdef NAUT_CONFIG_DEBUG_SCHED
+#if DUMP_SCHED_STATE
 #define DUMP_RT_PENDING(s,p) rt_priority_queue_dump(&(s)->pending,p)
+#else
+#define DUMP_RT_PENDING(s,p)
+#endif
+#else
+#define DUMP_RT_PENDING(s,p)
+#endif
 
 #define GET_NEXT_RT(s)   rt_priority_queue_dequeue(&(s)->runnable)
 #define PUT_RT(s,t) rt_priority_queue_enqueue(&(s)->runnable,t)
 #define REMOVE_RT(s,t) rt_priority_queue_remove(&(s)->runnable,t)
 #define HAVE_RT(s) (!rt_priority_queue_empty(&(s)->runnable))
 #define PEEK_RT(s) (s->runnable.threads[0])
+#ifdef NAUT_CONFIG_DEBUG_SCHED
+#if DUMP_SCHED_STATE
 #define DUMP_RT(s,p) rt_priority_queue_dump(&(s)->runnable,p)
+#else
+#define DUMP_RT(s,p)
+#endif
+#else
+#define DUMP_RT(s,p)
+#endif
 
 //
 // Per-thread state (abstracted from overall thread context
@@ -397,7 +437,7 @@ static void           set_timer(rt_scheduler *scheduler,
 				rt_thread *thread, 
 				uint64_t now);
 
-static void           handle_special_switch(rt_status what, int have_lock, uint8_t flags);
+static void           handle_special_switch(rt_status what, int have_lock, uint8_t flags, spinlock_t *lock_to_release);
 
 static inline uint64_t get_min_per(rt_priority_queue *runnable, rt_priority_queue *queue, rt_thread *thread);
 static inline uint64_t get_avg_per(rt_priority_queue *runnable, rt_priority_queue *pending, rt_thread *thread);
@@ -628,13 +668,19 @@ struct nk_thread *nk_find_thread_by_tid(uint64_t tid)
     return q.thread;
 }
 
-void nk_sched_reap()
+void nk_sched_reap(int uncond)
 {
     GLOBAL_LOCK_CONF;
     struct sys_info *sys = per_cpu_get(system);
     struct apic_dev *apic = sys->cpus[my_cpu_id()]->apic;
     struct rt_list *rlist;
 
+    if (!uncond && global_sched_state.num_threads < ((NAUT_CONFIG_MAX_THREADS * 95)/100)) {
+	// unless we are unconditionally reaping, do not reap if we still
+	// have a lot of threads left....
+	return;
+    }
+
     if (!__sync_bool_compare_and_swap(&global_sched_state.reaping,0,1)) {
 	// reaping is already in progress elsewhere
 	return;
@@ -725,6 +771,8 @@ static int initial_placement(nk_thread_t *t)
 int nk_sched_thread_post_create(nk_thread_t * t)
 {
     GLOBAL_LOCK_CONF;
+
+    nk_sched_reap(0); // conditional reap to make room for new thread
     
     t->current_cpu = initial_placement(t);
 
@@ -983,9 +1031,9 @@ int nk_sched_make_runnable(struct nk_thread *thread, int cpu, int admit)
 }
 
 
-void nk_sched_exit()
+void nk_sched_exit(spinlock_t *lock_to_release)
 {
-    handle_special_switch(EXITING,0,0);
+    handle_special_switch(EXITING,0,0,lock_to_release);
     // we should not come back!
     panic("Returned to finished thread!\n");
 }
@@ -1475,16 +1523,27 @@ static inline void set_interrupt_priority(rt_thread *t)
 // Invoked in interrupt context by the timer interrupt or
 // some other interrupt
 //
+// Invoked in non-interrupt context by handle_special_switch
+//
 // Returns NULL if there is no need to change the current thread
 // Returns pointer to the thread to switch to otherwise
 //
 // In both cases updates the timer to reflect the thread
 // that should be running
 //
-struct nk_thread *_sched_need_resched(int have_lock)
+struct nk_thread *_sched_need_resched(int have_lock, int force_resched)
 {
     LOCAL_LOCK_CONF;
 
+    if (preempt_is_disabled())  {
+	if (force_resched) {
+	    DEBUG("Forced reschedule with preemption off\n");
+	} else {
+	    DEBUG("Preemption disabled, avoiding rescheduling pass and staying with current thread\n");
+	    return 0;
+	}
+    }
+
     INST_SCHED_IN();
 
     uint64_t now = cur_time();
@@ -1599,6 +1658,7 @@ struct nk_thread *_sched_need_resched(int have_lock)
 	    // current aperiodic thread has run out of time or is yielding
 	    // keep it on the aperiodic run queue
 	    //DEBUG_DUMP(rt_c,CUR_NOT_SPECIAL_STR);
+	    DEBUG("Set current aperiodic thread (%lu) to suspended\n",rt_c->thread->tid);
 	    rt_c->thread->status=NK_THR_SUSPENDED;
 	    if (PUT_APERIODIC(scheduler, rt_c)) { 
 		goto panic_queue;
@@ -1924,7 +1984,7 @@ struct nk_thread *_sched_need_resched(int have_lock)
 	
     default:
 	ERROR("Unknown current task type %d... just letting it run\n",rt_c->constraints.type);
-	return 0;
+	goto out_good_early;
 	break;
     }    
     
@@ -1980,13 +2040,14 @@ struct nk_thread *_sched_need_resched(int have_lock)
     set_timer(scheduler, rt_n, now);
     if (rt_n!=rt_c) {
 	if (rt_n->thread->status==NK_THR_RUNNING) { 
-	    ERROR("Switching to new thread that is already running (old tid=%llu\n",rt_c->thread->tid);
+	    ERROR("Switching to new thread that is already running (old tid=%llu (%s), new tid=%llu (%s))\n",rt_c->status,rt_c->thread->tid,rt_c->thread->name,rt_n->thread->tid,rt_n->thread->name);
 	    DUMP_ENTRY_CONTEXT();
 	}
 	// We may have switched away from a thread attempting to sleep
 	// before we were able to do so, in which case, don't reset its status
 	// we need to try again
 	if (rt_n->thread->status != NK_THR_WAITING) { 
+	    DEBUG("Switching new thread (%lu) from state %u to state %u\n",rt_n->thread->tid, rt_n->thread->status, NK_THR_RUNNING);
 	    rt_n->thread->status=NK_THR_RUNNING;
 	}
 	// The currently running thread has either been marked suspended
@@ -2023,10 +2084,11 @@ struct nk_thread *_sched_need_resched(int have_lock)
 	// the thread may be marked waiting as we may have been preempted in the 
 	// middle of going to sleep
 	if (rt_c->thread->status!=NK_THR_SUSPENDED && rt_c->thread->status!=NK_THR_WAITING && !yielding) { 
-	    ERROR("Staying with current thread, but it is not marked suspended or waiting or yielding (thread status is %u rt thread status is %u tid=%llu)\n",rt_c->thread->status,rt_c->status,rt_c->thread->tid);
+	    ERROR("Staying with current thread, but it is not marked suspended or waiting or yielding (thread status is %u rt thread status is %u tid=%llu (%s))\n",rt_c->thread->status,rt_c->status,rt_c->thread->tid,rt_c->thread->name);
 	    DUMP_ENTRY_CONTEXT();
 	}
-	if (rt_c->thread->status!=NK_THR_WAITING) { 
+	if (rt_c->thread->status!= NK_THR_WAITING) { 
+	    DEBUG("Switching current thread (%lu) from state %u to state %u\n",rt_c->thread->tid,rt_c->thread->status, NK_THR_RUNNING);
 	    rt_c->thread->status=NK_THR_RUNNING;
 	}
 
@@ -2043,7 +2105,7 @@ struct nk_thread *_sched_need_resched(int have_lock)
 
 struct nk_thread *nk_sched_need_resched()
 {
-    return _sched_need_resched(0);
+    return _sched_need_resched(0,0);
 }
 
 int nk_sched_thread_change_constraints(struct nk_sched_constraints *constraints)
@@ -2080,7 +2142,7 @@ int nk_sched_thread_change_constraints(struct nk_sched_constraints *constraints)
 	}
 	// we are now on the aperiodic run queue
 	// so we need to get running again with our new constraints
-	handle_special_switch(CHANGING,1,_local_flags);
+	handle_special_switch(CHANGING,1,_local_flags,0);
 	// we've now released the lock, so reacquire
 	LOCAL_LOCK(scheduler);
     }
@@ -2107,7 +2169,7 @@ int nk_sched_thread_change_constraints(struct nk_sched_constraints *constraints)
 	// we are now aperioidic
 	// since we are again on the run queue
 	// we need to kick ourselves off the cp
-	handle_special_switch(CHANGING,1,_local_flags);
+	handle_special_switch(CHANGING,1,_local_flags,0);
 	// when we come back, we note that we have failed
 	// we also have no lock
 	goto out_bad_no_unlock;
@@ -2119,7 +2181,7 @@ int nk_sched_thread_change_constraints(struct nk_sched_constraints *constraints)
 	DUMP_RT_PENDING(scheduler,"pending before handle special switch");
 	DUMP_APERIODIC(scheduler,"aperiodic before handle special switch");
 
-	handle_special_switch(CHANGING,1,_local_flags);
+	handle_special_switch(CHANGING,1,_local_flags,0);
 
 	// we now have released lock and interrupts are back to prior
 
@@ -2376,8 +2438,18 @@ extern void nk_thread_switch(nk_thread_t*);
 // before return if the lock is held
 // prior to any thread switch, the lock is released
 // interrupt state restoration operates just as any other switch
-static void handle_special_switch(rt_status what, int have_lock, uint8_t flags)
+// have_lock implies we have the lock and we have disabled interrupts, 
+// with flags being the restore interrupt value
+ static void handle_special_switch(rt_status what, int have_lock, uint8_t flags, spinlock_t *lock_to_release)
 {
+    int did_preempt_disable = 0;
+    int no_switch=0;
+
+    if (!preempt_is_disabled()) {
+	preempt_disable();
+	did_preempt_disable = 1;
+    }
+
     if (!have_lock) { 
 	// we always want a local critical section here
 	flags = irq_disable_save();
@@ -2405,8 +2477,14 @@ static void handle_special_switch(rt_status what, int have_lock, uint8_t flags)
 
     c->sched_state->status = what;
 
+    // force a rescheduling pass even though preemption is off
+    n = _sched_need_resched(have_lock,1);
 
-    n = _sched_need_resched(have_lock);
+    // at this point, the scheduler has been updated and so we can
+    // release the user's lock / flags
+    if (lock_to_release) { 
+	spin_unlock(lock_to_release);
+    }
 
     if (!n) {
 	switch (what) {
@@ -2427,12 +2505,13 @@ static void handle_special_switch(rt_status what, int have_lock, uint8_t flags)
 	    ERROR("Huh - unknown request %d in handle_special_switch()\n",what);
 	    break;
 	}
+	no_switch = 1;
 	goto out_good;
     }
 
 #ifdef NAUT_CONFIG_ENABLE_STACK_CHECK
-    if (c->rsp <= (uint64_t)(c->stack)) {
-	panic("This thread (%p, tid=%u) has run off the end of its stack! (start=%p, rsp=%p, start size=%lx)\n", 
+    if ((c->rsp <= (uint64_t)(c->stack)) || c->rsp >= (uint64_t)(c->stack+c->stack_size)) {
+	panic("This thread (%p, tid=%u) has run off the end (or beginning) of its stack! (start=%p, rsp=%p, start size=%lx)\n", 
 	      (void*)c,
 	      c->tid,
 	      c->stack,
@@ -2453,9 +2532,15 @@ static void handle_special_switch(rt_status what, int have_lock, uint8_t flags)
 	have_lock = 0;
     }
 
+    // Now preemption is enabled, but interrupts are 
+    // still off, so we will continue to run to completion
+    preempt_reset();
+
     // at this point, we have interrupts off
     // whatever we switch to will turn them back on
-
+    // our context will indicate interrupts off
+    // when we switch away, we will leave rflags.if=0 on
+    // the stack and preemption enabled
     nk_thread_switch(n);
     
     DEBUG("After return from switch (back in %llu \"%s\")\n", c->tid, c->name);
@@ -2465,6 +2550,13 @@ static void handle_special_switch(rt_status what, int have_lock, uint8_t flags)
     if (have_lock) {
 	spin_unlock(&s->lock);
     }
+    if (no_switch && did_preempt_disable) { 
+	// we are not doing a context switch, so we need to revert
+	// preemption state to what we had on entry
+	// if we had done a context switch, the preemption state
+	// would have been reset before the switch back to us
+	preempt_enable();
+    }
     // and now we restore the interrupt state to 
     // what we had on entry
     irq_enable_restore(flags);
@@ -2478,9 +2570,9 @@ static void handle_special_switch(rt_status what, int have_lock, uint8_t flags)
  *
  */
 void 
-nk_sched_yield(void)
+nk_sched_yield(spinlock_t *lock_to_release)
 {
-    handle_special_switch(YIELDING,0,0);
+    handle_special_switch(YIELDING,0,0,lock_to_release);
 }
 
 
@@ -2489,12 +2581,13 @@ nk_sched_yield(void)
  *
  * unconditionally schedule some other thread
  * a thread sleeps only if it wants to stop being runnable
- *
+ * the thread can pass in a lock to release once
+ * the scheduling pass is complete
  */
 void 
-nk_sched_sleep(void)
+nk_sched_sleep(spinlock_t *lock_to_release)
 {
-    handle_special_switch(SLEEPING,0,0);
+    handle_special_switch(SLEEPING,0,0,lock_to_release);
 }
 
 static int rt_thread_check_deadlines(rt_thread *t, rt_scheduler *s, uint64_t now)
@@ -2597,7 +2690,6 @@ static uint64_t cur_time()
 {
     struct sys_info *sys = per_cpu_get(system);
     struct apic_dev *apic = sys->cpus[my_cpu_id()]->apic;
-
     return apic_cycles_to_realtime(apic, rdtsc());
 }
 
@@ -2741,11 +2833,11 @@ static int rt_thread_admit(rt_scheduler *scheduler, rt_thread *thread, uint64_t
 	  thread->constraints.type==APERIODIC ? "Aperiodic" :
 	  thread->constraints.type==PERIODIC ? "Periodic" :
 	  thread->constraints.type==SPORADIC ? "Sporadic" : "Unknown",
-	  thread->constraints.interrupt_priorty_class,
+	  thread->constraints.interrupt_priority_class,
 	  util_limit,aper_res,spor_res,per_res);
 
     if (thread->constraints.interrupt_priority_class > 0xe) {
-	DEBUG("Rejecting thread with too high of an interrupt priorty class (%u)\n", thread->constraints.interrupt_priority_class);
+	DEBUG("Rejecting thread with too high of an interrupt priority class (%u)\n", thread->constraints.interrupt_priority_class);
 	return -1;
     }
 	
@@ -3005,6 +3097,7 @@ static int shared_init(struct cpu *my_cpu, struct nk_sched_config *cfg)
 
     ZERO(main);
 
+
     // need to be sure this is aligned, hence direct use of malloc here
     my_stack = malloc(PAGE_SIZE);
 
@@ -3021,6 +3114,16 @@ static int shared_init(struct cpu *my_cpu, struct nk_sched_config *cfg)
 	goto fail_free;
     }
 
+    // Since this is an already running "thread", it already has a stack.
+    // When we return to either init() or to smp_ap_entry(), a stack
+    // switch will be performed to the stack we just allocated.  
+    // It is possible that init() or smp_ap_entry()
+    // will end up popping stuff off of what the compiler thinks is the 
+    // *same* stack throughout.  The result is tha we could underrun the new stack
+    // and corrupt adjacent memory at a higher address.
+    // We guard against this by padding the stack pointer 
+    main->rsp -= 1024;
+
     main->bound_cpu = my_cpu_id(); // idle threads cannot move
     main->status = NK_THR_RUNNING;
     main->sched_state->status = ADMITTED;
@@ -3035,6 +3138,8 @@ static int shared_init(struct cpu *my_cpu, struct nk_sched_config *cfg)
 
     put_cur_thread(main);
 
+    my_cpu->sched_state->current = main->sched_state;
+
     nk_sched_thread_post_create(main);
 
 #ifdef NAUT_CONFIG_INTERRUPT_THREAD_ALLOW_IDLE
@@ -3144,7 +3249,7 @@ static void reaper(void *in, void **out)
 	DEBUG("Reaper sleeping\n");
 	nk_sleep(NAUT_CONFIG_AUTO_REAP_PERIOD_MS*1000000ULL);	
 	DEBUG("Reaping threads\n");
-	nk_sched_reap();
+	nk_sched_reap(1); // unconditional reap
     }
 }
 
diff --git a/src/nautilus/thread.c b/src/nautilus/thread.c
index b54f63c..767b394 100644
--- a/src/nautilus/thread.c
+++ b/src/nautilus/thread.c
@@ -50,7 +50,6 @@ extern uint8_t malloc_cpus_ready;
 
 static unsigned long next_tid = 0;
 
-
 extern addr_t boot_stack_start;
 extern void nk_thread_switch(nk_thread_t*);
 extern void nk_thread_entry(void *);
@@ -89,57 +88,39 @@ nk_thread_queue_destroy (nk_thread_queue_t * q)
 }
 
 
-
-static inline void 
-enqueue_thread_on_waitq (nk_thread_t * waiter, nk_thread_queue_t * waitq)
-{
-    ASSERT(waiter->status != NK_THR_WAITING);
-
-    waiter->status = NK_THR_WAITING;
-
-    nk_enqueue_entry_atomic(waitq, &(waiter->wait_node));
-}
-
-static inline nk_thread_t*
-dequeue_thread_from_waitq (nk_thread_t * waiter, nk_thread_queue_t * waitq)
-{
-    nk_thread_t * t        = NULL;
-    nk_queue_entry_t * elm = nk_dequeue_entry_atomic(waitq, &(waiter->wait_node));
-
-    t = container_of(elm, nk_thread_t, wait_node);
-
-    if (t) {
-        t->status = NK_THR_SUSPENDED;
-    }
-
-    return t;
-}
-
-
-
-
-
 /*
  * thread_detach
  *
  * detaches a child from its parent
  *
- * assumes interrupts are off
- *
  * @t: the thread to detach
  *
  */
 static int
 thread_detach (nk_thread_t * t)
 {
+    preempt_disable();
+
     ASSERT(t->refcount > 0);
 
     /* remove me from my parent's child list */
     list_del(&(t->child_node));
 
-    if (--t->refcount == 0) {
-        nk_thread_destroy(t);
-    }
+    --t->refcount;
+
+    // conditional reaping is done by the scheduler when threads are created
+    // this makes the join+exit path much faster in the common case and 
+    // bulks reaping events together
+    // the user can also explictly reap when needed
+    // plus the autoreaper thread can be enabled 
+    // the following code can be enabled if you want to reap immediately once
+    // a thread's refcount drops to zero
+    // 
+    //if (t->refcount==0) { 
+    //   nk_thread_destroy(t);
+    //}
+
+    preempt_enable();
 
     return 0;
 }
@@ -443,6 +424,9 @@ int nk_thread_run(nk_thread_id_t t)
   
   thread_setup_init_stack(newthread, newthread->fun, newthread->input);
   
+
+  THREAD_DEBUG("Run thread initialized: %p (tid=%lu) stack=%p size=%lu rsp=%p\n",newthread,newthread->tid,newthread->stack,newthread->stack_size,newthread->rsp);
+
   if (nk_sched_make_runnable(newthread, newthread->current_cpu,1)) { 
       THREAD_ERROR("Scheduler failed to run thread (%p, tid=%u) on cpu %u\n",
 		  newthread, newthread->tid, newthread->current_cpu);
@@ -486,10 +470,61 @@ nk_wake_waiters (void)
 
 void nk_yield()
 {
-    //    THREAD_DEBUG("NK YIELD!\n");
-    nk_sched_yield();
+    struct nk_thread *me = get_cur_thread();
+
+    spin_lock(&me->lock);
+
+    nk_sched_yield(&me->lock);
 }
 
+
+// defined early to allow inlining - this is used in multiple places
+static void _thread_queue_wake_all (nk_thread_queue_t * q, int have_lock)
+{
+    nk_queue_entry_t * elm = NULL;
+    nk_thread_t * t = NULL;
+    uint8_t flags=0;
+
+    if (in_interrupt_context()) {
+	THREAD_DEBUG("[Interrupt Context] Thread %lu (%s) is waking all waiters on thread queue (q=%p)\n", get_cur_thread()->tid, get_cur_thread()->name, (void*)q);
+    } else {
+	THREAD_DEBUG("[Thread Context] Thread %lu (%s) is waking all waiters on thread queue (q=%p)\n", get_cur_thread()->tid, get_cur_thread()->name, (void*)q);
+    }
+
+    ASSERT(q);
+
+    if (!have_lock) { 
+	flags = spin_lock_irq_save(&q->lock);
+    }
+
+    THREAD_DEBUG("Wakeup: have lock\n");
+
+    while ((elm = nk_dequeue_first(q))) {
+        t = container_of(elm, nk_thread_t, wait_node);
+
+	THREAD_DEBUG("Waking %lu (%s), status %lu\n", t->tid,t->name,t->status);
+
+        ASSERT(t);
+	ASSERT(t->status == NK_THR_WAITING);
+
+	if (nk_sched_awaken(t, t->current_cpu)) { 
+	    THREAD_ERROR("Failed to awaken thread\n");
+	    goto out;
+	}
+
+	nk_sched_kick_cpu(t->current_cpu);
+	THREAD_DEBUG("Waking all waiters on thread queue (q=%p) woke thread %lu (%s)\n", (void*)q,t->tid,t->name);
+
+    }
+
+ out:
+    THREAD_DEBUG("Wakeup complete - releasing lock\n");
+    if (!have_lock) {
+	spin_unlock_irq_restore(&q->lock, flags);
+    }
+}
+
+
 /*
  * nk_thread_exit
  *
@@ -502,34 +537,55 @@ void nk_yield()
  * any destructors for thread local storage
  *
  */
-void
-nk_thread_exit (void * retval) 
+void nk_thread_exit (void * retval) 
 {
     nk_thread_t * me = get_cur_thread();
+    nk_thread_queue_t * wq = me->waitq;
+    uint8_t flags;
 
-    /* clear any thread local storage that may have been allocated */
-    tls_exit();
-    
-    while (__sync_lock_test_and_set(&me->lock, 1));
+    THREAD_DEBUG("Thread %p (tid=%u (%s)) exiting, joining with children\n", me, me->tid, me->name);
 
     /* wait for my children to finish */
     nk_join_all_children(NULL);
 
+    THREAD_DEBUG("Children joined\n");
+
+    /* clear any thread local storage that may have been allocated */
+    tls_exit();
+
+    THREAD_DEBUG("TLS exit complete\n");
+
+    // lock out anyone else looking at my wait queue
+    // we need to do this before we change our own state
+    // so we can avoid racing with someone who is attempting
+    // to join with us and is putting themselves on our wait queue
+    flags = spin_lock_irq_save(&wq->lock);
+    preempt_disable();
+    irq_enable_restore(flags);   // we only need interrupts off long enough to lock out the scheduler
+
+    THREAD_DEBUG("Lock acquired\n");
+
+    // at this point, we have the lock on our wait queue and preemption is disabled
+
     me->output      = retval;
     me->status      = NK_THR_EXITED;
 
-    /* wake up everyone who is waiting on me */
-    nk_wake_waiters();
+    // force arch and compiler to do above writes now
+    __asm__ __volatile__ ("mfence" : : : "memory"); 
 
-    me->refcount--;
+    THREAD_DEBUG("State update complete\n");
 
-    THREAD_DEBUG("Thread %p (tid=%u) exiting, joining with children\n", me, me->tid);
+    /* wake up everyone who is waiting on me */
+    _thread_queue_wake_all(wq, 1);
 
-    __sync_lock_release(&me->lock);
+    THREAD_DEBUG("Waiting wakeup complete\n");
 
-    cli();
+    me->refcount--;
+
+    THREAD_DEBUG("Thread %p (tid=%u (%s)) exit complete - invoking scheduler\n", me, me->tid, me->name);
 
-    nk_sched_exit();
+    // the scheduler will reenable preemption and release the lock
+    nk_sched_exit(&wq->lock);
 
     /* we should never get here! */
     panic("Should never get here!\n");
@@ -540,7 +596,6 @@ nk_thread_exit (void * retval)
  * nk_thread_destroy
  *
  * destroys a thread and reclaims its memory (its stack page mostly)
- * interrupts should be off
  *
  * @t: the thread to destroy
  *
@@ -552,7 +607,7 @@ nk_thread_destroy (nk_thread_id_t t)
 
     THREAD_DEBUG("Destroying thread (%p, tid=%lu)\n", (void*)thethread, thethread->tid);
 
-    ASSERT(!irqs_enabled());
+    preempt_disable();
 
     nk_sched_thread_pre_destroy(thethread);
 
@@ -566,9 +621,20 @@ nk_thread_destroy (nk_thread_id_t t)
     nk_sched_thread_state_deinit(thethread);
     free(thethread->stack);
     free(thethread);
+    
+    preempt_enable();
 }
 
 
+static int exit_check(void *state)
+{
+    volatile nk_thread_t *thethread = (nk_thread_t *)state;
+    
+    THREAD_DEBUG("exit_check: thread (%lu %s) status is %u\n",thethread->tid,thethread->name,thethread->status);
+    return thethread->status==NK_THR_EXITED;
+}
+    
+
 /*
  * nk_join
  *
@@ -585,39 +651,30 @@ int
 nk_join (nk_thread_id_t t, void ** retval)
 {
     nk_thread_t *thethread = (nk_thread_t*)t;
-    uint8_t flags;
+
+    THREAD_DEBUG("Join initiated for thread %lu \"%s\"\n", thethread->tid, thethread->name);
 
     ASSERT(thethread->parent == get_cur_thread());
 
-    flags = irq_disable_save();
+    nk_thread_queue_sleep_extended(thethread->waitq, exit_check, thethread);
 
-    while (__sync_lock_test_and_set(&thethread->lock, 1));
+    THREAD_DEBUG("Join commenced for thread %lu \"%s\"\n", thethread->tid, thethread->name);
 
-    if (thethread->status == NK_THR_EXITED) {
-        if (thethread->output) {
-            *retval = thethread->output;
-        }
-        goto out;
-    } else {
-        while (*(volatile int*)&thethread->status != NK_THR_EXITED) {
-            __sync_lock_release(&thethread->lock);
-            cli();
-            nk_wait(t);
-            sti();
-            while (__sync_lock_test_and_set(&thethread->lock, 1));
-        }
-    }
+    ASSERT(exit_check(thethread));
 
     if (retval) {
         *retval = thethread->output;
     }
 
-out:
-    __sync_lock_release(&thethread->lock);
     thread_detach(thethread);
-    irq_enable_restore(flags);
+
+    THREAD_DEBUG("Join completed for thread %lu \"%s\"\n", thethread->tid, thethread->name);
+    
     return 0;
 }
+    
+
+
 
 
 /* 
@@ -652,7 +709,7 @@ nk_join_all_children (int (*func)(void * res))
 
         if (func) {
             if (func(res) < 0) {
-                THREAD_ERROR("Could not invoke destructo for child thread (t=%p)\n", elm);
+                THREAD_ERROR("Consumer indicated error for child thread (t=%p, output=%p)\n", elm,res);
                 ret = -1;
                 continue;
             }
@@ -664,34 +721,6 @@ nk_join_all_children (int (*func)(void * res))
 }
 
 
-/* 
- * nk_wait
- *
- * Go to sleep on a thread's wait queue. 
- *
- * @t : the thread to wait on
- *
- */
-void
-nk_wait (nk_thread_id_t t)
-{
-    nk_thread_t * cur    = get_cur_thread();
-    nk_thread_t * waiton = (nk_thread_t*)t;
-
-    nk_thread_queue_t * wq = waiton->waitq;
-
-    /* make sure we're not putting ourselves on our 
-     * own waitq */
-    ASSERT(!irqs_enabled());
-    ASSERT(wq != cur->waitq);
-
-    enqueue_thread_on_waitq(cur, wq);
-
-    nk_sched_sleep();
-    
-}
-
-
 
 
 /* 
@@ -709,29 +738,82 @@ nk_set_thread_fork_output (void * result)
 
 
 /*
- * nk_thread_queue_sleep
+ * nk_thread_queue_sleep_extended
  *
- * Goes to sleep on the given queue
+ * Goes to sleep on the given queue, checking a condition as it does so
  *
  * @q: the thread queue to sleep on
- * 
+ * @cond_check - condition to check (return nonzero if true) atomically with queuing
+ * @state - state for cond_check
+ *  
  */
-int 
-nk_thread_queue_sleep (nk_thread_queue_t * q)
+void nk_thread_queue_sleep_extended(nk_thread_queue_t *wq, int (*cond_check)(void *state), void *state)
 {
     nk_thread_t * t = get_cur_thread();
+    uint8_t flags;
 
-    THREAD_DEBUG("SLEEP ON WAIT QUEUE\n");
+    THREAD_DEBUG("Thread %lu (%s) going to sleep on queue %p\n", t->tid, t->name, (void*)wq);
 
-    enqueue_thread_on_waitq(t, q);
+    // grab control over the the wait queue
+    flags = spin_lock_irq_save(&wq->lock);
 
-    __asm__ __volatile__ ("" : : : "memory");
-    
-    nk_sched_sleep();
+    // at this point, any waker is either about to start on the
+    // queue or has just finished  It's possible that
+    // we have raced with with it and it has just finished
+    // we therefore need to double check the condition now
 
-    THREAD_DEBUG("WAKE UP FROM WAIT QUEUE\n");
+    if (cond_check && cond_check(state)) { 
+	// The condition we are waiting on has been achieved
+	// already.  The waker is either done waking up
+	// threads or has not yet started.  In either case
+	// we do not want to queue ourselves
+	spin_unlock_irq_restore(&wq->lock, flags);
+	THREAD_DEBUG("Thread %lu (%s) has fast wakeup on queue %p - condition already met\n", t->tid, t->name, (void*)wq);
+	return;
+    } else {
+	// the condition still is not signalled 
+	// or the condition is not important, therefore
+	// while still holding the lock, put ourselves on the 
+	// wait queue
+
+	THREAD_DEBUG("Thread %lu (%s) is queueing itself on queue %p\n", t->tid, t->name, (void*)wq);
+	
+	t->status = NK_THR_WAITING;
+	nk_enqueue_entry(wq, &(t->wait_node));
+
+	// force arch and compiler to do above writes
+	__asm__ __volatile__ ("mfence" : : : "memory"); 
+
+	// disallow the scheduler from context switching this core
+	// until we (in particular nk_sched_sleep()) decide otherwise
+	preempt_disable(); 
+
+	// reenable local interrupts - the scheduler is still blocked
+	// because we have preemption off
+	// any waker at this point will still get stuck on the wait queue lock
+	// it will be a short spin, hopefully
+	irq_enable_restore(flags);
+	
+	THREAD_DEBUG("Thread %lu (%s) is having the scheduler put itself to sleep on queue %p\n", t->tid, t->name, (void*)wq);
+
+	// We now get the scheduler to do a context switch
+	// and just after it completes its scheduling pass, 
+	// it will release the wait queue lock for us
+	// it will also reenable preemption on its context switch out
+	nk_sched_sleep(&wq->lock);
+	
+	THREAD_DEBUG("Thread %lu (%s) has slow wakeup on queue %p\n", t->tid, t->name, (void*)wq);
+
+	// note no spin_unlock here since nk_sched_sleep will have 
+	// done it for us
+
+	return;
+    }
+}
 
-    return 0;
+void nk_thread_queue_sleep(nk_thread_queue_t *wq)
+{
+    return nk_thread_queue_sleep_extended(wq,0,0);
 }
 
 
@@ -742,17 +824,18 @@ nk_thread_queue_sleep (nk_thread_queue_t * q)
  *
  * @q: the queue to use
  *
- * returns -EINVAL on error, 0 on success
- *
  */
-int 
-nk_thread_queue_wake_one (nk_thread_queue_t * q)
+void nk_thread_queue_wake_one (nk_thread_queue_t * q)
 {
     nk_queue_entry_t * elm = NULL;
     nk_thread_t * t = NULL;
     uint8_t flags = irq_disable_save();
 
-    THREAD_DEBUG("Thread queue wake one (q=%p)\n", (void*)q);
+    if (in_interrupt_context()) {
+	THREAD_DEBUG("[Interrupt Context] Thread %lu (%s) is waking one waiter on thread queue (q=%p)\n", get_cur_thread()->tid, get_cur_thread()->name, (void*)q);
+    } else {
+	THREAD_DEBUG("Thread %lu (%s) is waking one waiter on thread queue (q=%p)\n", get_cur_thread()->tid, get_cur_thread()->name, (void*)q);
+    }
 
     ASSERT(q);
 
@@ -776,13 +859,16 @@ nk_thread_queue_wake_one (nk_thread_queue_t * q)
 
     nk_sched_kick_cpu(t->current_cpu);
 
+    THREAD_DEBUG("Thread queue wake one (q=%p) work up thread %lu (%s)\n", (void*)q, t->tid, t->name);
+
 out:
     irq_enable_restore(flags);
 
-    return 0;
 }
 
 
+
+
 /* 
  * nk_thread_queue_wake_all
  *
@@ -793,39 +879,11 @@ out:
  * returns -EINVAL on error, 0 on success
  *
  */
-int
-nk_thread_queue_wake_all (nk_thread_queue_t * q)
+void nk_thread_queue_wake_all (nk_thread_queue_t * q)
 {
-    nk_queue_entry_t * elm = NULL;
-    nk_thread_t * t = NULL;
-    uint8_t flags;
-
-    THREAD_DEBUG("Waking all waiters on thread queue (q=%p)\n", (void*)q);
-
-    ASSERT(q);
-
-    flags = spin_lock_irq_save(&q->lock);
-
-    while ((elm = nk_dequeue_first(q))) {
-        t = container_of(elm, nk_thread_t, wait_node);
-
-        ASSERT(t);
-        ASSERT(t->status == NK_THR_WAITING);
-
-	if (nk_sched_awaken(t, t->current_cpu)) { 
-	    THREAD_ERROR("Failed to awaken thread\n");
-	    goto out;
-	}
-
-	nk_sched_kick_cpu(t->current_cpu);
-    }
-
- out:
-    spin_unlock_irq_restore(&q->lock, flags);
-    return 0;
+    _thread_queue_wake_all(q,0);
 }
 
-
 /* 
  * nk_tls_key_create
  *
@@ -983,9 +1041,10 @@ nk_get_parent_tid (void)
 // prior to the current stack frame, at least.
 // should be at least 16
 #define LAUNCHPAD 16
+// Attempt to clone this many frames when doing a fork
+// If these cannot be resolved correctly, then only a single
+// frame is cloned
 #define STACK_CLONE_DEPTH 2
-#define STACK_SIZE_MIN    (4096 * 16)
-#define LAUNCHER_STACK_SIZE STACK_SIZE_MIN
 
 /* 
  * note that this isn't called directly. It is vectored
@@ -996,11 +1055,27 @@ nk_get_parent_tid (void)
 nk_thread_id_t 
 __thread_fork (void)
 {
+    nk_thread_t *parent = get_cur_thread();
     nk_thread_id_t  tid;
     nk_thread_t * t;
     nk_stack_size_t size, alloc_size;
     uint64_t     rbp1_offset_from_ret0_addr;
     void         *child_stack;
+    uint64_t     rsp;
+
+    __asm__ __volatile__ ( "movq %%rsp, %0" : "=r"(rsp) : : "memory");
+
+#ifdef NAUT_CONFIG_ENABLE_STACK_CHECK
+    // now check again after update to see if we didn't overrun/underrun the stack in the parent... 
+    if ((uint64_t)rsp <= (uint64_t)parent->stack ||
+	(uint64_t)rsp >= (uint64_t)(parent->stack + parent->stack_size)) { 
+	THREAD_ERROR("Parent's top of stack (%p) exceeds boundaries of stack (%p-%p)\n",
+		     rsp, parent->stack, parent->stack+parent->stack_size);
+	panic("Detected stack out of bounds in parent during fork\n");
+    }
+#endif
+
+    THREAD_DEBUG("Forking thread from parent=%p tid=%lu stack=%p-%p rsp=%p\n", parent, parent->tid,parent->stack,parent->stack+parent->stack_size,rsp);
 
 #ifdef NAUT_CONFIG_THREAD_OPTIMIZE 
     THREAD_WARN("Thread fork may function incorrectly with aggressive threading optimizations\n");
@@ -1011,27 +1086,32 @@ __thread_fork (void)
     void *rbp_tos   = __builtin_frame_address(STACK_CLONE_DEPTH);   // should scan backward to avoid having this be zero or crazy
     void *ret0_addr = rbp0 + 8;
 
-
     // we're being called with a stack not as deep as STACK_CLONE_DEPTH...
     // fail back to a single frame...
-    if (rbp_tos == 0 || rbp_tos < rbp1) { 
+    if ((uint64_t)rbp_tos <= (uint64_t)parent->stack ||
+	(uint64_t)rbp_tos >= (uint64_t)(parent->stack + parent->stack_size)) { 
+	THREAD_DEBUG("Cannot resolve %lu stack frames on fork, using just one\n", STACK_CLONE_DEPTH);
         rbp_tos = rbp1;
     }
 
-    // from last byte of tos_rbp to the last byte of the stack on return from this function (return address of wrapper
+
+    // from last byte of tos_rbp to the last byte of the stack on return from this function 
+    // (return address of wrapper)
     // the "launch pad" is added so that in the case where there is no stack frame above the caller
     // we still have the space to fake one.
     size = (rbp_tos + 8) - ret0_addr + LAUNCHPAD;   
 
+    //THREAD_DEBUG("rbp0=%p rbp1=%p rbp_tos=%p, ret0_addr=%p\n", rbp0, rbp1, rbp_tos, ret0_addr);
+
     rbp1_offset_from_ret0_addr = rbp1 - ret0_addr;
 
-    alloc_size = (size > STACK_SIZE_MIN) ? size : STACK_SIZE_MIN;    // at least enough to grow to STACK_SIZE_MIN
+    alloc_size = parent->stack_size;
 
     if (nk_thread_create(NULL,        // no function pointer, we'll set rip explicity in just a sec...
                          NULL,        // no input args, it's not a function
                          NULL,        // no output args
                          0,           // this thread's parent will wait on it
-                         TSTACK_2MB,  // stack size
+                         alloc_size,  // stack size
                          &tid,        // give me a thread id
                          CPU_ANY)     // not bound to any particular CPU
             < 0) {
@@ -1041,11 +1121,17 @@ __thread_fork (void)
 
     t = (nk_thread_t*)tid;
 
+    THREAD_DEBUG("Forked thread created: %p (tid=%lu) stack=%p size=%lu rsp=%p\n",t,t->tid,t->stack,t->stack_size,t->rsp);
+
     child_stack = t->stack;
 
     // this is at the top of the stack, just in case something goes wrong
     thread_push(t, (uint64_t)&thread_cleanup);
 
+    //THREAD_DEBUG("child_stack=%p, alloc_size=%lu size=%lu\n",child_stack,alloc_size,size);
+    //THREAD_DEBUG("copy to %p-%p from %p\n", child_stack + alloc_size - size,
+    //             child_stack + alloc_size - size + size - LAUNCHPAD, ret0_addr);
+
     // Copy stack frames of caller and up to stack max
     // this should copy from 1st byte of my_rbp to last byte of tos_rbp
     // notice that leaves ret
@@ -1065,12 +1151,33 @@ __thread_fork (void)
     // we provide null for thread func to indicate this is a fork
     thread_setup_init_stack(t, NULL, NULL); 
 
+    THREAD_DEBUG("Forked thread initialized: %p (tid=%lu) stack=%p size=%lu rsp=%p\n",t,t->tid,t->stack,t->stack_size,t->rsp);
+
+#ifdef NAUT_CONFIG_ENABLE_STACK_CHECK
+    // now check the child before we attempt to run it
+    if ((uint64_t)t->rsp <= (uint64_t)t->stack ||
+	(uint64_t)t->rsp >= (uint64_t)(t->stack + t->stack_size)) { 
+	THREAD_ERROR("Child's rsp (%p) exceeds boundaries of stack (%p-%p)\n",
+		     t->rsp, t->stack, t->stack+t->stack_size);
+	panic("Detected stack out of bounds in child during fork\n");
+	return 0;
+    }
+#endif
+
+#ifdef NAUT_CONFIG_FPU_SAVE
+    // clone the floating point state
+    extern void nk_fp_save(void *dest);
+    nk_fp_save(t->fpu_state);
+#endif
+
     if (nk_sched_make_runnable(t,t->current_cpu,1)) { 
 	THREAD_ERROR("Scheduler failed to run thread (%p, tid=%u) on cpu %u\n",
 		    t, t->tid, t->current_cpu);
 	return 0;
     }
 
+    THREAD_DEBUG("Forked thread made runnable: %p (tid=%lu)\n",t,t->tid);
+
     // return child's tid to parent
     return tid;
 }
diff --git a/src/nautilus/timer.c b/src/nautilus/timer.c
index c2f0a43..4cfef4c 100644
--- a/src/nautilus/timer.c
+++ b/src/nautilus/timer.c
@@ -141,13 +141,19 @@ int nk_cancel_timer(struct nk_timer *t)
     return 0;
 }
 
+static int check(void *state)
+{
+    struct nk_timer *t = state;
+    return __sync_fetch_and_add(&t->signaled,0);
+}
+
 int nk_wait_timer(struct nk_timer *t)
 {
     DEBUG("Wait timer %p\n",t);
     while (!__sync_fetch_and_add(&t->signaled,0)) {
 	if (!(t->flags & TIMER_SPIN)) { 
 	    DEBUG("Going to sleep on thread queue\n");
-	    nk_thread_queue_sleep(t->waitq);
+	    nk_thread_queue_sleep_extended(t->waitq, check, t);
 	} else {
 	    asm volatile ("pause");
 	}
-- 
1.9.1

